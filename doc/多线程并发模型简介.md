# 多线程并发模型简介

## 常见的多线程/多进程条件下的并发模型

- 半同步半反应堆模式，也可以叫做one loop per thread
- 半同步半异步模式，也叫做HSHA模式

两种模型的实现方法有很多相似之处，比如

- 采用io复用函数监听所有连接套接字，在Linux下通常是epoll
- 采用固定大小线程池

## 半同步半反应堆模型

正如被称作one loop per thread模式，该模式下每个线程都运行着一个事件循环，在每个事件循环下都采用一个io复用函数负责监听这个线程上的所有连接

对于连接的分配根据实现会有多种方法

### 主线程仅仅负责接收客户端连接请求

接收后将这个连接分发给某个loop，然后继续监听。这种方式下主线程的事件循环仅仅负责监听监听套接字的可读事件。由于网络库背后通常都需要保存每个连接的对象（通常是智能指针），所以采用这种方式可以保证添加连接对象的操作是同步的，只在主线程下执行，不会出现竞争问题，但是删除对象时会有这方面的问题，需要加锁解决

### 每个线程都会单独创建监听套接字监听目标端口

连接对象仅在接收它的哪个线程创建和销毁，连接对象的保存和销毁不存在资源竞争的问题

但是当一个连接请求到达时所有线程的监听套接字都会变成可读，导致此时所有线程的io复用都被唤醒，但是只能有一个线程成功接收，其它线程都在做无用功，产生著名的“惊群”效应

解决方法有两种

#### 保证同一时刻只有一个线程的监听套接字被添加到io复用中

进入io等待函数(如epoll_wait)前执行锁争抢，成功抢到锁的线程将监听套接字添加到io复用中。此外也可以根据当前线程的负载选择是否争抢锁，同时也解决了负载均衡的问题，不会出现一个线程处理的连接对象特别多而另一个线程处理的连接对象特别少的情况

#### 设置监听套接字的REUSE_PORT选项

REUSE_PORT选项是Linux内核为了解决负载均衡和惊群问题时引入的功能，当创建多个套接字，并且这些套接字都需要监听同一端口时，可以为每个套接字设置这个选项，当有新连接到达时，Linux会选择负载较轻的那个监听套接字，将连接请求传入



## 半同步半异步模型

相比前一种，这种模型的设计相对简单，一个主线程负责监听所有连接请求以及连接对象上的io事件，当要执行事件处理函数时，将这个函数传给线程池，线程池中维护一个生产者消费者模型，每个线程都从任务队列中取出任务函数执行

这种模型仍然存在某些问题，比如对于同一个连接对象上的io事件，某一时刻它既可读又可写，那么很有可能读事件处理函数和写事件处理函数由线程池中两个不同线程执行，倘若其中一个处理函数将连接套接字关闭，那另一个处理函数必然出错。解决办法就是使用epoll的ONESHOT选项，保证同一时刻只能激活一个io事件

## 目前了解到的采用上述模型的网络架构

- muduo，采用半同步半反应堆模型，每个线程一个事件循环，主线程接收连接请求分发给其它线程
- nginx，采用半同步半反应堆模型，每个进程一个事件循环，每个进程都有各自的监听套接字并设置REUSE_PORT选项



## 线程池的设计

在半同步半异步模式下，线程池相当于一个生产者消费者模式，主线程作为生产者，负责将io执行函数添加到线程池的任务队列中，线程池中的线程作为消费者，负责从任务队列中取出任务执行

所以线程池中需要保存一个任务队列，当执行添加/取出操作时，通过加锁的方式避免资源竞争

每个线程的运行流程

- 获取互斥锁
- 如果任务队列为空，则通过条件变量进入睡眠（该步会自动释放已获得的互斥锁）
- 如果任务队列非空，取任务，解锁，执行任务



涉及到C++11的线程库知识点

- 线程库std::thread
- 互斥锁std::mutex, std::lock_guard, std::unique_lock
- 条件变量std::conditional_variable
- 任务包装库std::packaged_task
- 异步结果获取std::future

编译链接库：-std=c++11 -lpthread

涉及到C++17的知识点：模板参数自动推导（如果使用c++17，需要GCC7.1版本以上，链接时使用-std=c++17）

```c++

class threadpool
{
    public:
        using Task = std::function<void()>;
		
		...
		
        template <typename Func, typename... Args>
        auto async(Func&& f, Args&&... args)
            -> std::future<std::invoke_result_t<Func(Args...)>>
        {
            using return_type = std::future<std::invoke_result_t<Func(Args...)>>;
            auto task = std::make_shared<std::packaged_task<return_type()>>(
                std::bind(std::forward<Func>(f), std::forward<Args>(args)...));
            auto ret = task->get_future();
            {
                std::unique_lock lock{ mutex_ };
                tasks_.emplace([task]{ (*task)(); });
                cond_.notify_one();
            }
            return ret;
        }
    private:
        std::vector<std::thread> threads_;
        std::queue<Task> tasks_;
        std::mutex mutex_;
        std::condition_variable cond_;
};
```



线程池完整代码[参考这里](https://github.com/rocwangp/cortono/blob/master/util/threadpool.hpp)



